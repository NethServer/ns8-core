#!/usr/bin/env python3

#
# Copyright (C) 2021 Nethesis S.r.l.
# http://www.nethesis.it - nethserver@nethesis.it
#
# This script is part of NethServer.
#
# NethServer is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License,
# or any later version.
#
# NethServer is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with NethServer.  If not, see COPYING.
#

import os
import sys
import json
import agent
import agent.tasks
import ipcalc
import uuid
import cluster.grants

# Action summary:
# 1. allocate a VPN IP address for the node
# 2. create redis acls for the node agent

request = json.load(sys.stdin)

node_pwh = request['node_pwh']
agent.assert_exp(node_pwh and len(node_pwh) == 64)
public_key = request['public_key']
agent.assert_exp(public_key)
endpoint = request['endpoint']
agent.assert_exp(type(endpoint) is str)
listen_port = request['listen_port']
agent.assert_exp(listen_port > 0)

rdb = agent.redis_connect(privileged=True)

#
# 1/a. Retrieve the leader VPN attributes,
#      required by the caller to complete the join-cluster action
#
leader_id = rdb.hget('cluster/environment', 'NODE_ID')
leader_ip_address = rdb.hget(f'node/{leader_id}/vpn', 'ip_address')
agent.assert_exp(leader_ip_address)
leader_public_key = rdb.hget(f'node/{leader_id}/vpn', 'public_key')
agent.assert_exp(leader_public_key)
leader_listen_port = rdb.hget(f'node/{leader_id}/vpn', 'listen_port')
agent.assert_exp(leader_listen_port)
leader_endpoint = rdb.hget(f'node/{leader_id}/vpn', 'endpoint')
agent.assert_exp(leader_endpoint)

#
# 1/b. Generate a new node_id and VPN IP address
#
network = ipcalc.Network(rdb.get('cluster/network'))
node_id = int(rdb.incr(f'cluster/node_sequence'))
ip_address = str(network.network() + node_id)

# Prepare a list of tasks to update VPN routes on existing nodes
update_routes_tasks = [{
    'agent_id': knode.removesuffix('/vpn'),
    'action': 'update-routes',
    'data': {},
} for knode in rdb.scan_iter('node/*/vpn')] # It's ok, the new node record is still not present.

#
# 1/c. Store VPN settings of the new node
#
agent.assert_exp(rdb.hset(f'node/{node_id}/vpn', mapping={
    "public_key": public_key,
    "ip_address": ip_address,
    "destinations": "",
    "endpoint": endpoint,
    "listen_port": str(listen_port),
}) >= 0)

# Initialize the node ports sequence
agent.assert_exp(rdb.set(f'node/{node_id}/tcp_ports_sequence', 20000) is True)

#
# 2. Create redis acls for the node agent
#
agent.assert_exp(rdb.execute_command('ACL', 'SETUSER',
    f'node/{node_id}', 'ON', '#' + node_pwh,
    'resetkeys', f'~node/{node_id}/*',
    'resetchannels', f'&progress/node/{node_id}/*', f'&node/{node_id}/event/*',
    'nocommands', '+@read', '+@write', '+@transaction', '+@connection', '+publish',
) == 'OK')

#
# 3. Ask existing nodes (leader included) to update their IP routes and VPN configuration
#
node_errors = agent.tasks.runp_brief(
    update_routes_tasks,
    endpoint="redis://cluster-leader",
)
agent.assert_exp(node_errors == 0)

#
# 4. Additional tasks for the new node: they are picked up when the join-cluster has been completed
#    in a random order and must not depend on each other.
#
additional_tasks = [
    {
        # Remove the default traefik1 instance from the worker node. This
        # task races against "add-module traefik", however it should
        # always win because the latter has to download the Traefik image
        # and in the worst case, the new Traefik instance will start a
        # crash-loop until ports 80 and 443 are freed.
        'agent_id': f'node/{node_id}',
        'action': 'remove-module',
        'data': {
            "module_id": 'traefik1',
            "preserve_data": False,
        }
    },
    {
        'agent_id': 'cluster',
        'action': 'add-module',
        'data': {
            "image": f'ghcr.io/nethserver/traefik:latest',
            "node": node_id,
        }
    },
    {
        'agent_id': 'cluster',
        'action': 'add-module',
        'data': {
            "image": f'ghcr.io/nethserver/ldapproxy:latest',
            "node": node_id,
        }
    },
    {
        'agent_id': f'node/{node_id}',
        'action': 'update-routes',
        'data': {},
    },
    {
        #
        # Node API authorizations (call blocked on node/{node_id}/tasks queue)
        #
        'agent_id': 'cluster',
        'action': 'grant-actions',
        'data': [
            {"action": "*", "to": "owner", "on": f'node/{node_id}'},
            {"action": "list-*", "to": "reader", "on": f'node/{node_id}'},
            {"action": "get-*", "to": "reader", "on": f'node/{node_id}'},
            {"action": "show-*", "to": "reader", "on": f'node/{node_id}'},
            {"action": "read-*", "to": "reader", "on": f'node/{node_id}'},
        ]
    },
]
subtasks = agent.tasks.runp_nowait(
    additional_tasks,
    endpoint="redis://cluster-leader",
)

# Grant the owner role to cluster owners on the new node.
# cluster.grants.alter_user() cannot be invoked here because roles are
# still not created by grant-actions.  However at this point node_id, role
# and user are known so the relation can be fully established.
for userk in rdb.scan_iter('roles/*'):
    user_auths = rdb.hgetall(userk)
    if 'cluster' in user_auths and user_auths['cluster'] == 'owner':
        rdb.hset(userk, f'node/{node_id}', 'owner')

agent.save_acls(rdb)

#
# Return the new information to the caller
#
json.dump({
    "node_id": node_id, # required to set up node agent
    "ip_address": ip_address, # required to set up wg0 device
    "leader_public_key": leader_public_key, # required for VPN auth
    "network": str(network), # required to set up route table
    "leader_ip_address": leader_ip_address, # required to rewrite /etc/hosts cluster-leader
    "leader_endpoint": leader_endpoint, # required to reach the WireGuard listen_port
    "subtasks": subtasks,
}, sys.stdout)
