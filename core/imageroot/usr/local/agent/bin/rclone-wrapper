#!/usr/bin/env python3

#
# Copyright (C) 2023 Nethesis S.r.l.
# SPDX-License-Identifier: GPL-3.0-or-later
#

import sys
import agent
import os
from urllib.parse import urlparse

def extract_region_code(hostname, position, default=""):
    """Obtain the region code for S3 backends by looking at the domain
    parts of the endpoint FQDN."""
    try:
        return hostname.split('.')[position]
    except ValueError:
        return default

core_env = agent.read_envfile("/etc/nethserver/core.env")
rclone_image = core_env["RESTIC_IMAGE"]

rdb = agent.redis_connect(privileged=False, host="127.0.0.1")
try:
    repository_id = sys.argv[1]
except IndexError:
    print("Usage:", file=sys.stderr)
    print("   rclone-wrapper {REPOSITORY_ID|BACKUP_ID} [rclone args...]", file=sys.stderr)
    print("", file=sys.stderr)
    print("The string \"REMOTE_PATH\" is replaced with the value defined by REPOSITORY_ID", file=sys.stderr)
    print("e.g.: rclone-wrapper 1 ls REMOTE_PATH/dokuwiki", file=sys.stderr)
    sys.exit(33)

if repository_id == "0":
    # Read orepo attributes directly from the environment.
    orepo = os.environ
elif repository_id.isnumeric():
    # Assume a backup ID has been issued. Translate it to a repository ID.
    repository_id = rdb.hget(f"cluster/backup/{repository_id}", "repository")
    orepo = rdb.hgetall(f"cluster/backup_repository/{repository_id}")
else:
    # Read orepo from Redis.
    orepo = rdb.hgetall(f"cluster/backup_repository/{repository_id}")

if not orepo:
    print(f"Could not find any repo with {sys.argv[1]}", file=sys.stderr)
    sys.exit(34)

# Parse URL and prepare RCLONE_* environment variables
uscheme, upath = orepo['url'].split(':', 1)

if uscheme == "b2":
    rclone_path = ':b2:' + upath
    rclone_env = {
        'RCLONE_B2_ACCOUNT': orepo["b2_account_id"],
        'RCLONE_B2_KEY': orepo["b2_account_key"],
    }
elif uscheme == "s3":
    s3_endpoint, s3_path = upath.split('/', 1)
    rclone_path = ':s3:' + s3_path
    rclone_env = {
        'RCLONE_S3_ENV_AUTH': 'true',
        'RCLONE_S3_ACCESS_KEY_ID': orepo["aws_access_key_id"],
        'RCLONE_S3_SECRET_ACCESS_KEY': orepo["aws_secret_access_key"],
        'RCLONE_S3_ENDPOINT': s3_endpoint
    }
    if orepo['provider'] == 'aws':
        rclone_env['RCLONE_S3_PROVIDER'] = 'AWS'
        rclone_env['RCLONE_S3_REGION'] = orepo.get("aws_default_region", "")
    elif orepo['provider'] == 'generic-s3' and 'digitalocean' in s3_endpoint:
        rclone_env['RCLONE_S3_PROVIDER'] = 'DigitalOcean'
    elif orepo['provider'] == 'generic-s3' and 'ovh.net' in s3_endpoint:
        rclone_env['RCLONE_S3_PROVIDER'] = 'Other'
        rclone_env['RCLONE_S3_REGION'] = orepo.get("aws_default_region", extract_region_code(s3_endpoint, 1))
        rclone_env['RCLONE_S3_LOCATION_CONSTRAINT'] = rclone_env['RCLONE_S3_REGION']
    elif orepo['provider'] == 'generic-s3' and 'wasabi' in s3_endpoint:
        rclone_env['RCLONE_S3_PROVIDER'] = 'Wasabi'
        rclone_env['RCLONE_S3_REGION'] = orepo.get("aws_default_region", extract_region_code(s3_endpoint, 1))
    elif orepo['provider'] == 'generic-s3' and 'synology' in s3_endpoint:
        rclone_env['RCLONE_S3_PROVIDER'] = 'Synology'
        rclone_env['RCLONE_S3_REGION'] = orepo.get("aws_default_region", extract_region_code(s3_endpoint, 0))
        rclone_env['RCLONE_S3_NO_CHECK_BUCKET'] = "1"
    elif orepo['provider'] == 'generic-s3' and 'idrivee2' in s3_endpoint:
        rclone_env['RCLONE_S3_PROVIDER'] = 'IDrive'
        rclone_env['RCLONE_S3_NO_CHECK_BUCKET'] = "1"
    else:
        rclone_env['RCLONE_S3_PROVIDER'] = 'Other'

elif uscheme == "azure":
    rclone_path = ':azureblob:' + upath.rstrip(":")
    rclone_env = {
        'RCLONE_AZUREBLOB_ACCOUNT': orepo["azure_account_name"],
        'RCLONE_AZUREBLOB_KEY': orepo["azure_account_key"],
    }
elif uscheme == "smb":
    rclone_path = ':smb:' + upath.rstrip(":")
    rclone_env = {
        "RCLONE_SMB_HOST": orepo["smb_host"],
        "RCLONE_SMB_USER": orepo["smb_user"],
        "RCLONE_SMB_PASS": orepo["smb_pass"],
        "RCLONE_SMB_DOMAIN": orepo["smb_domain"],
    }
elif uscheme == "webdav":
    ourl = urlparse(upath)
    rclone_path = ':webdav:' + ourl.path.rstrip("/")
    rclone_env = {
        "RCLONE_WEBDAV_URL": ourl.scheme + '://' + ourl.netloc
    }
else:
    raise Exception(f"Scheme {uscheme} not supported")

# Build the Podman+Rclone command line
container_name = "rclone-wrapper-" + os.environ.get('MODULE_ID', os.environ["AGENT_ID"]) + "-" + str(os.getpid())
exec_args = [
    "podman", "run",
    "-i", "--attach=stdin", "--attach=stdout", "--attach=stderr",
    "--log-driver=none",
    "--env=RCLONE*", "--network=host", "--rm",
    "--name=" + container_name,
    '--entrypoint=["/usr/local/bin/rclone-wrapper"]',
    rclone_image,
    ] + sys.argv[2:]

# Substitute REMOTE_PATH placeholder in Rclone args
exec_args = [rarg.replace('REMOTE_PATH', rclone_path) for rarg in exec_args]

if os.getenv('DEBUG', 0):
    print(*([f"{k}={v}" for k,v in rclone_env.items()] + exec_args), file=sys.stderr)

os.environ.update(rclone_env)
os.execvp("podman", exec_args)
